run_name: 'finetuning_set_bindingmoad_ca_cond_11052024_gcpnet'
logdir: '/bml/acmwhb/Repositories/Lab_Repositories/GCDM-SBDD/logs/train'
wandb_params:
  mode: 'online'  # disabled, offline, online
  entity: 'bml-lab'
dataset: 'bindingmoad'  # NOTE: this is used as a safe choice for dataset metadata
datadir: '/bml/acmwhb/Repositories/Lab_Repositories/GCDM-SBDD/data/finetuning_set_processed_noH_ca_only/'
enable_progress_bar: False
num_sanity_val_steps: 0

mode: 'pocket_conditioning'
pocket_representation: 'CA'
batch_size: 1
accumulate_grad_batches: 1
lr: 1.0e-4
n_epochs: 100
num_workers: 1
gpus: 1
clip_grad: True
augment_rotation: False
augment_noise: 0
strategy: ddp_find_unused_parameters_true

auxiliary_loss: False
loss_params:
  max_weight: 1.0
  schedule: 'linear'
  clamp_lj: 3.0

net_dynamics_params:
  model_name: gcpnet
  device: 'cuda'
  edge_cutoff: null
  joint_nf: 32
  hidden_nf: 256
  n_layers: 6
  attention: True
  tanh: True
  norm_constant: 1
  inv_sublayers: 1
  sin_embedding: False
  aggregation_method: 'sum'
  normalization_factor: 100  # used if aggregation_method='sum'

diffusion_params:
  diffusion_steps: 1000
  diffusion_noise_schedule: 'polynomial_2'
  diffusion_noise_precision: 1.0e-5
  diffusion_loss_type: 'l2'
  normalize_factors: [1, 4]  # [x, h]

eval_epochs: 25
visualize_sample_epoch: 25
visualize_chain_epoch: 25
eval_params:
  n_eval_samples: 100
  eval_batch_size: 100
  smiles_file: '/bml/acmwhb/Repositories/Lab_Repositories/GCDM-SBDD/data/finetuning_set_processed_noH_ca_only/train_smiles.npy'
  n_visualize_samples: 5
  keep_frames: 100